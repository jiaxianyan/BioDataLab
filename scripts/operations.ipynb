{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from deep_data_research.commons.utils import pmap_multi\n",
    "from openai import OpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_SECRET_KEY = \"sk-di6EQiNDxyGzzoFa93389fA755B14eAfAf31B863F2E6C369\"\n",
    "BASE_URL = \"https://aihubmix.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_SECRET_KEY\n",
    "os.environ[\"OPENAI_BASE_URL\"] = BASE_URL\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_SECRET_KEY\n",
    "os.environ[\"GOOGLE_BASE_URL\"] = BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_english_into_chinese(text):\n",
    "    client = OpenAI(base_url=BASE_URL, api_key=API_SECRET_KEY)\n",
    "    response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": 'Please translate the following english text into chinese:\\n' + text}], \n",
    "                                                        model='gemini-2.0-flash', temperature=0)\n",
    "    output = response.choices[0].message.content\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "translating chinese to english: 0it [00:00, ?it/s][Parallel(n_jobs=256)]: Using backend LokyBackend with 256 concurrent workers.\n",
      "translating chinese to english: 10it [00:00, 600.53it/s]\n",
      "[Parallel(n_jobs=256)]: Done   5 out of  10 | elapsed:   59.9s remaining:   59.9s\n",
      "[Parallel(n_jobs=256)]: Done  10 out of  10 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'MinerU_output/NAR_cetergory/RNA sequence databases'\n",
    "pdf_dirs = os.listdir(data_dir)\n",
    "\n",
    "texts = []\n",
    "for pdf_dir in pdf_dirs:\n",
    "    text_md = os.path.join(data_dir, pdf_dir, 'auto', f'{pdf_dir}.md')\n",
    "    with open(text_md, 'r') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)\n",
    "\n",
    "outputs = pmap_multi(translate_english_into_chinese, zip(texts), n_jobs=256, desc='translating chinese to english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_dir, output in zip(pdf_dirs, outputs):\n",
    "    text_cn_md = os.path.join(data_dir, pdf_dir, 'auto', f'{pdf_dir}_cn.md')\n",
    "    with open(text_cn_md, 'w') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_chinese_content(text):\n",
    "    client = OpenAI(base_url=BASE_URL, api_key=API_SECRET_KEY)\n",
    "    response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": '下面是一个关于数据集的论文，请你帮我总结论文，以下面的形式\\n任务目标：<构建这个数据集的目的是什么，为什么要构建这个数据集>\\n详细解释：<详细解释任务目标，帮助我们更加的理解，包括对一些会出现的科学概念的解释>\\n构建过程：<具体的构建步骤>，目的是什么，输入是什么，输出是什么，如何实现的(用的什么工具，什么操作)>\\n下面是论文具体的内容：' + text}], \n",
    "                                                        model='gemini-2.5-flash-lite-preview-06-17', temperature=0)\n",
    "    output = response.choices[0].message.content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "summary chinese content: 0it [00:00, ?it/s][Parallel(n_jobs=256)]: Using backend LokyBackend with 256 concurrent workers.\n",
      "summary chinese content: 9it [00:00, 7980.71it/s]\n",
      "[Parallel(n_jobs=256)]: Done   9 out of   9 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "pdf_dirs = os.listdir(data_dir)\n",
    "texts = []\n",
    "for pdf_dir in pdf_dirs:\n",
    "    text_md = os.path.join(data_dir, pdf_dir, 'auto', f'{pdf_dir}_cn.md')\n",
    "    with open(text_md, 'r') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)\n",
    "\n",
    "outputs = pmap_multi(summary_chinese_content, zip(texts), n_jobs=256, desc='summary chinese content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_dir, output in zip(pdf_dirs, outputs):\n",
    "    text_cn_md = os.path.join(data_dir, pdf_dir, 'auto', f'{pdf_dir}_cn_summary.md')\n",
    "    with open(text_cn_md, 'w') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_data_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c676a7d98e4aa41edde794fecb383fcc36d19fdc817b33c3ad371454bbfe9cbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
