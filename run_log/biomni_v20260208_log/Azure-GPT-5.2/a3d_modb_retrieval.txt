Query the UniProt database to find all reviewed (Swiss-Prot) proteins from Escherichia coli (strain K12) that are annotated with the Protein name "transmembrane". 

Save the list of their UniProt accession numbers to `/root/biodatalab/log/biomni_v20260209_pred_results/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval.json`.

Output example format:
```json
[
  "P0A8T7",
  "P0A8T8",
  "P0A8T9",
  ...
]
```

All intermediate results should be saved to `/root/biodatalab/log/biomni_v20260209_tmp/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval`

<execute>
import os, json, textwrap, time
from pathlib import Path
import requests

# Research log helper
def log(msg):
    print(f"[LOG] {msg}")

# Paths
tmp_dir = Path("/root/biodatalab/log/biomni_v20260209_tmp/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval")
out_path = Path("/root/biodatalab/log/biomni_v20260209_pred_results/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval.json")
tmp_dir.mkdir(parents=True, exist_ok=True)
out_path.parent.mkdir(parents=True, exist_ok=True)

plan = [
    "[ ] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields",
    "[ ] Query UniProt REST API and page through results; save raw responses to tmp",
    "[ ] Parse accession numbers; deduplicate and validate format; save parsed intermediates to tmp",
    "[ ] Write final JSON list to requested output path"
]
print("Plan:")
for i,p in enumerate(plan,1):
    print(f"{i}. {p}")

# Step 1: build query
query = 'reviewed:true AND organism_id:83333 AND protein_name:transmembrane'
fields = "accession"
format_ = "tsv"
size = 500  # pagination size

step1 = {
    "query": query,
    "fields": fields,
    "format": format_,
    "size": size,
    "endpoint": "https://rest.uniprot.org/uniprotkb/search"
}
(step1_path := tmp_dir/"step1_query.json").write_text(json.dumps(step1, indent=2))
log(f"Saved query definition to {step1_path}")

plan[0] = "[✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields"
print("\nUpdated plan:")
for i,p in enumerate(plan,1):
    print(f"{i}. {p}")

# Step 2: request and paginate
session = requests.Session()
headers = {"User-Agent": "biodatalab-uniprot-query/1.0"}
params = {"query": query, "format": format_, "fields": fields, "size": size}

all_tsv_chunks = []
all_links = []
url = step1["endpoint"]
page = 1

while True:
    log(f"Fetching page {page} ...")
    r = session.get(url, params=params, headers=headers, timeout=60)
    (tmp_dir/f"raw_page_{page}.status.txt").write_text(f"{r.status_code}\n")
    r.raise_for_status()
    tsv_text = r.text
    (tmp_dir/f"raw_page_{page}.tsv").write_text(tsv_text)
    all_tsv_chunks.append(tsv_text)
    link = r.headers.get("Link")
    all_links.append(link)
    (tmp_dir/f"raw_page_{page}.headers.json").write_text(json.dumps(dict(r.headers), indent=2))
    # UniProt pagination uses 'Link: <next>; rel="next"'
    next_url = None
    if link and 'rel="next"' in link:
        # parse next url
        parts = [p.strip() for p in link.split(",")]
        for p in parts:
            if 'rel="next"' in p:
                next_url = p[p.find("<")+1:p.find(">")]
                break
    if not next_url:
        log("No next link; pagination complete.")
        break
    # next_url already includes query; for subsequent requests don't pass params
    url = next_url
    params = None
    page += 1
    time.sleep(0.2)

(step2_path := tmp_dir/"step2_pagination_links.json").write_text(json.dumps(all_links, indent=2))
log(f"Saved pagination link headers to {step2_path}")

plan[1] = "[✓] Query UniProt REST API and page through results; save raw responses to tmp"
print("\nUpdated plan:")
for i,p in enumerate(plan,1):
    print(f"{i}. {p}")

# Step 3: parse accessions
accessions = []
for chunk in all_tsv_chunks:
    lines = [ln for ln in chunk.splitlines() if ln.strip()]
    # first line is header "Entry"
    for ln in lines[1:]:
        acc = ln.split("\t")[0].strip()
        if acc:
            accessions.append(acc)

# deduplicate while preserving order
seen = set()
uniq = []
for a in accessions:
    if a not in seen:
        uniq.append(a)
        seen.add(a)

# validate rough UniProt accession patterns (not exhaustive)
import re
pat = re.compile(r"^[A-NR-Z0-9]{1}[0-9][A-Z0-9]{3}[0-9]$|^[OPQ][0-9][A-Z0-9]{3}[0-9]$|^[A-NR-Z0-9]{6,10}$")
valid = [a for a in uniq if pat.match(a)]
invalid = [a for a in uniq if a not in valid]

(step3a := tmp_dir/"step3_accessions_raw.json").write_text(json.dumps(accessions, indent=2))
(step3b := tmp_dir/"step3_accessions_unique.json").write_text(json.dumps(uniq, indent=2))
(step3c := tmp_dir/"step3_accessions_invalid.json").write_text(json.dumps(invalid, indent=2))
log(f"Parsed {len(accessions)} accessions, {len(uniq)} unique; invalid={len(invalid)}")
log(f"Example unique accessions: {uniq[:10]}")

plan[2] = "[✓] Parse accession numbers; deduplicate and validate format; save parsed intermediates to tmp"
print("\nUpdated plan:")
for i,p in enumerate(plan,1):
    print(f"{i}. {p}")

# Step 4: write final JSON
out_path.write_text(json.dumps(valid, indent=2) + "\n")
(step4 := tmp_dir/"step4_written_output_path.txt").write_text(str(out_path) + "\n")
log(f"Wrote {len(valid)} accession numbers to {out_path}")

plan[3] = "[✓] Write final JSON list to requested output path"
print("\nUpdated plan:")
for i,p in enumerate(plan,1):
    print(f"{i}. {p}")

# Print final output preview
print("\nFinal JSON preview (first 20):")
print(json.dumps(valid[:20], indent=2))
</execute>
<observation>NameError("name 'pat' is not defined")</observation>
<execute>
import os, json, time
from pathlib import Path
import requests
import re

def log(msg):
    print(f"[LOG] {msg}")

tmp_dir = Path("/root/biodatalab/log/biomni_v20260209_tmp/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval")
out_path = Path("/root/biodatalab/log/biomni_v20260209_pred_results/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval.json")
tmp_dir.mkdir(parents=True, exist_ok=True)
out_path.parent.mkdir(parents=True, exist_ok=True)

plan = [
    "[ ] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields",
    "[ ] Query UniProt REST API and page through results; save raw responses to tmp",
    "[ ] Parse accession numbers; deduplicate and validate format; save parsed intermediates to tmp",
    "[ ] Write final JSON list to requested output path"
]
print("Plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 1) Define query
query = 'reviewed:true AND organism_id:83333 AND protein_name:transmembrane'
fields = "accession"
format_ = "tsv"
size = 500
endpoint = "https://rest.uniprot.org/uniprotkb/search"

step1 = {"endpoint": endpoint, "query": query, "fields": fields, "format": format_, "size": size}
(step1_path := tmp_dir/"step1_query.json").write_text(json.dumps(step1, indent=2))
log(f"Saved query definition to {step1_path}")

plan[0] = "[✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 2) Fetch and paginate
session = requests.Session()
headers = {"User-Agent": "biodatalab-uniprot-query/1.0"}
params = {"query": query, "format": format_, "fields": fields, "size": size}

all_tsv_chunks = []
all_links = []
url = endpoint
page = 1

while True:
    log(f"Fetching page {page} ...")
    r = session.get(url, params=params, headers=headers, timeout=60)
    (tmp_dir/f"raw_page_{page}.status.txt").write_text(f"{r.status_code}\n")
    r.raise_for_status()

    tsv_text = r.text
    (tmp_dir/f"raw_page_{page}.tsv").write_text(tsv_text)
    (tmp_dir/f"raw_page_{page}.headers.json").write_text(json.dumps(dict(r.headers), indent=2))
    all_tsv_chunks.append(tsv_text)

    link = r.headers.get("Link")
    all_links.append(link)

    next_url = None
    if link and 'rel="next"' in link:
        for part in [p.strip() for p in link.split(",")]:
            if 'rel="next"' in part:
                next_url = part[part.find("<")+1:part.find(">")]
                break

    if not next_url:
        log("No next link; pagination complete.")
        break

    url = next_url
    params = None
    page += 1
    time.sleep(0.2)

(step2_path := tmp_dir/"step2_pagination_links.json").write_text(json.dumps(all_links, indent=2))
log(f"Saved pagination link headers to {step2_path}")

plan[1] = "[✓] Query UniProt REST API and page through results; save raw responses to tmp"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 3) Parse accessions; deduplicate; validate
accessions = []
for chunk in all_tsv_chunks:
    lines = [ln for ln in chunk.splitlines() if ln.strip()]
    if not lines:
        continue
    for ln in lines[1:]:  # skip header
        acc = ln.split("\t")[0].strip()
        if acc:
            accessions.append(acc)

seen = set()
uniq = []
for a in accessions:
    if a not in seen:
        uniq.append(a)
        seen.add(a)

# Define pat (fixing the observed NameError)
pat = re.compile(
    r"^(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z0-9]{6,10})$"
)
valid = [a for a in uniq if pat.match(a)]
invalid = [a for a in uniq if a not in valid]

(tmp_dir/"step3_accessions_raw.json").write_text(json.dumps(accessions, indent=2))
(tmp_dir/"step3_accessions_unique.json").write_text(json.dumps(uniq, indent=2))
(tmp_dir/"step3_accessions_invalid.json").write_text(json.dumps(invalid, indent=2))
log(f"Parsed {len(accessions)} accessions, {len(uniq)} unique; invalid={len(invalid)}")
log(f"Example unique accessions: {uniq[:10]}")

plan[2] = "[✓] Parse accession numbers; deduplicate and validate format; save parsed intermediates to tmp"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 4) Write final JSON list
out_path.write_text(json.dumps(valid, indent=2) + "\n")
(tmp_dir/"step4_written_output_path.txt").write_text(str(out_path) + "\n")
log(f"Wrote {len(valid)} accession numbers to {out_path}")

plan[3] = "[✓] Write final JSON list to requested output path"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

print("\nFinal JSON preview (first 20):")
print(json.dumps(valid[:20], indent=2))
</execute>
<observation>NameError("name 'pat' is not defined")</observation>
<execute>
import os, json, time, re, traceback
from pathlib import Path
import requests

def log(msg):
    print(f"[LOG] {msg}")

tmp_dir = Path("/root/biodatalab/log/biomni_v20260209_tmp/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval")
out_path = Path("/root/biodatalab/log/biomni_v20260209_pred_results/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval.json")
tmp_dir.mkdir(parents=True, exist_ok=True)
out_path.parent.mkdir(parents=True, exist_ok=True)

plan = [
    "[ ] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields",
    "[ ] Query UniProt REST API and page through results; save raw responses to tmp",
    "[ ] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)",
    "[ ] Write final JSON list to requested output path"
]
print("Plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 1) Define query
query = 'reviewed:true AND organism_id:83333 AND protein_name:transmembrane'
fields = "accession"
format_ = "tsv"
size = 500
endpoint = "https://rest.uniprot.org/uniprotkb/search"
(step1_path := tmp_dir/"step1_query.json").write_text(json.dumps(
    {"endpoint": endpoint, "query": query, "fields": fields, "format": format_, "size": size}, indent=2
))
log(f"Saved query definition to {step1_path}")

plan[0] = "[✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 2) Fetch and paginate
session = requests.Session()
headers = {"User-Agent": "biodatalab-uniprot-query/1.0"}
params = {"query": query, "format": format_, "fields": fields, "size": size}

all_tsv_chunks = []
all_links = []
url = endpoint
page = 1

while True:
    log(f"Fetching page {page} ...")
    r = session.get(url, params=params, headers=headers, timeout=60)
    (tmp_dir/f"raw_page_{page}.status.txt").write_text(f"{r.status_code}\n")
    r.raise_for_status()

    (tmp_dir/f"raw_page_{page}.tsv").write_text(r.text)
    (tmp_dir/f"raw_page_{page}.headers.json").write_text(json.dumps(dict(r.headers), indent=2))
    all_tsv_chunks.append(r.text)

    link = r.headers.get("Link")
    all_links.append(link)

    next_url = None
    if link and 'rel="next"' in link:
        for part in [p.strip() for p in link.split(",")]:
            if 'rel="next"' in part:
                next_url = part[part.find("<")+1:part.find(">")]
                break

    if not next_url:
        log("No next link; pagination complete.")
        break

    url = next_url
    params = None
    page += 1
    time.sleep(0.2)

(tmp_dir/"step2_pagination_links.json").write_text(json.dumps(all_links, indent=2))
log("Saved pagination links.")

plan[1] = "[✓] Query UniProt REST API and page through results; save raw responses to tmp"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 3) Parse accessions + deduplicate (avoid regex validation entirely)
accessions = []
for chunk in all_tsv_chunks:
    lines = [ln for ln in chunk.splitlines() if ln.strip()]
    if not lines:
        continue
    # header should be "Entry"
    for ln in lines[1:]:
        acc = ln.split("\t")[0].strip()
        if acc:
            accessions.append(acc)

seen = set()
uniq = []
for a in accessions:
    if a not in seen:
        uniq.append(a)
        seen.add(a)

(tmp_dir/"step3_accessions_raw.json").write_text(json.dumps(accessions, indent=2))
(tmp_dir/"step3_accessions_unique.json").write_text(json.dumps(uniq, indent=2))
log(f"Parsed {len(accessions)} accessions, {len(uniq)} unique.")
log(f"Example unique accessions: {uniq[:10]}")

plan[2] = "[✓] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

# 4) Write final JSON list
out_path.write_text(json.dumps(uniq, indent=2) + "\n")
(tmp_dir/"step4_written_output_path.txt").write_text(str(out_path) + "\n")
log(f"Wrote {len(uniq)} accession numbers to {out_path}")

plan[3] = "[✓] Write final JSON list to requested output path"
print("\nUpdated plan:")
for i, p in enumerate(plan, 1):
    print(f"{i}. {p}")

print("\nFinal JSON preview (first 20):")
print(json.dumps(uniq[:20], indent=2))
</execute>
<observation>Plan:
1. [ ] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields
2. [ ] Query UniProt REST API and page through results; save raw responses to tmp
3. [ ] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)
4. [ ] Write final JSON list to requested output path
[LOG] Saved query definition to /root/biodatalab/log/biomni_v20260209_tmp/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval/step1_query.json

Updated plan:
1. [✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields
2. [ ] Query UniProt REST API and page through results; save raw responses to tmp
3. [ ] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)
4. [ ] Write final JSON list to requested output path
[LOG] Fetching page 1 ...
[LOG] No next link; pagination complete.
[LOG] Saved pagination links.

Updated plan:
1. [✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields
2. [✓] Query UniProt REST API and page through results; save raw responses to tmp
3. [ ] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)
4. [ ] Write final JSON list to requested output path
[LOG] Parsed 3 accessions, 3 unique.
[LOG] Example unique accessions: ['P75958', 'P0ADC3', 'P75839']

Updated plan:
1. [✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields
2. [✓] Query UniProt REST API and page through results; save raw responses to tmp
3. [✓] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)
4. [ ] Write final JSON list to requested output path
[LOG] Wrote 3 accession numbers to /root/biodatalab/log/biomni_v20260209_pred_results/OpenAI/Azure-GPT-5.2/a3d_modb_retrieval.json

Updated plan:
1. [✓] Define UniProt query (reviewed, E. coli K-12, protein name contains 'transmembrane') and output fields
2. [✓] Query UniProt REST API and page through results; save raw responses to tmp
3. [✓] Parse accession numbers; deduplicate; save parsed intermediates to tmp (no regex validation to avoid NameError recurrence)
4. [✓] Write final JSON list to requested output path

Final JSON preview (first 20):
[
  "P75958",
  "P0ADC3",
  "P75839"
]
</observation>
<solution>
[
  "P75958",
  "P0ADC3",
  "P75839"
]
</solution>